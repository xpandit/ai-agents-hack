{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d3e6a46",
   "metadata": {},
   "source": [
    "# Challenge 5: Tool Usage & Agentic RAG\n",
    "\n",
    "In this challenge, we'll explore how to build an intelligent assistant that leverages Retrieval-Augmented Generation (RAG) in an agentic context. We'll create a system that can index company documentation, retrieve relevant information, and intelligently answer user queries.\n",
    "\n",
    "## What is Agentic RAG?\n",
    "\n",
    "Traditional RAG systems follow a linear process: retrieve relevant documents â†’ generate a response based on those documents. Agentic RAG takes this further by adding intelligent decision-making to the retrieval process:\n",
    "\n",
    "- **Dynamic Query Formulation**: The agent reformulates queries to improve search results\n",
    "- **Selective Retrieval**: The agent decides when to retrieve information and when to rely on its own knowledge\n",
    "- **Multi-step Reasoning**: The agent can perform multiple retrieval steps for complex questions\n",
    "- **Tool Integration**: The agent combines retrieval with other capabilities (calculations, API calls, etc.)\n",
    "\n",
    "## The Knowledge Base and Azure AI Search\n",
    "\n",
    "A knowledge base is a specialized database designed to store, organize, and retrieve information. In the context of AI applications:\n",
    "\n",
    "- **Knowledge bases** store structured or unstructured content (documents, FAQs, policies, etc.)\n",
    "- They're organized to facilitate quick and accurate information retrieval\n",
    "- They serve as the \"memory\" for AI agents, extending their knowledge beyond training data\n",
    "\n",
    "**Azure AI Search** (formerly Azure Cognitive Search) is Microsoft's cloud search service that enables:\n",
    "\n",
    "- **Document Ingestion**: Processing various file types (PDFs, Word, HTML, images with OCR, etc.)\n",
    "- **Indexing**: Creating searchable indexes with text analysis capabilities\n",
    "- **Semantic Search**: Using AI to understand query intent and contextual meaning\n",
    "- **Vector Search**: Utilizing embeddings to find conceptually similar content\n",
    "- **Hybrid Approaches**: Combining keyword and semantic search for optimal results\n",
    "\n",
    "In our agentic RAG system, Azure AI Search serves as the foundation for our knowledge base, enabling intelligent information retrieval to power our HR assistant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6762493d",
   "metadata": {},
   "source": [
    "## 1. Setting up Our Environment\n",
    "\n",
    "First, let's install the necessary packages for our Agentic RAG implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9400a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai python-dotenv azure-search-documents semantic-kernel azure-identity PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec54d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import uuid\n",
    "import PyPDF2\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Dict, Any, Annotated, Optional\n",
    "\n",
    "import asyncio\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SearchIndex, \n",
    "    SimpleField, \n",
    "    SearchFieldDataType, \n",
    "    SearchableField,\n",
    "    SearchField,\n",
    "    VectorSearch,\n",
    "    HnswAlgorithmConfiguration,\n",
    "    VectorSearchProfile,\n",
    "    VectorSearchAlgorithmKind\n",
    ")\n",
    "\n",
    "from openai import AsyncAzureOpenAI\n",
    "\n",
    "from semantic_kernel.kernel import Kernel\n",
    "from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion\n",
    "from semantic_kernel.contents.chat_history import ChatHistory\n",
    "from semantic_kernel.functions import kernel_function\n",
    "from semantic_kernel.functions.kernel_arguments import KernelArguments\n",
    "from semantic_kernel.connectors.ai import FunctionChoiceBehavior\n",
    "from semantic_kernel.contents.function_call_content import FunctionCallContent\n",
    "from semantic_kernel.contents.function_result_content import FunctionResultContent\n",
    "from semantic_kernel.agents import ChatCompletionAgent\n",
    "\n",
    "from IPython.display import display, HTML, Markdown\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9a89a3",
   "metadata": {},
   "source": [
    "## 2. Initializing Azure Services\n",
    "\n",
    "Now let's set up our connections to Azure AI Search and Azure OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfee88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Azure AI Search setup\n",
    "search_service_name = os.getenv(\"AZURE_SEARCH_SERVICE_NAME\")\n",
    "search_admin_key = os.getenv(\"AZURE_SEARCH_ADMIN_KEY\")\n",
    "search_endpoint = f\"https://{search_service_name}.search.windows.net\"\n",
    "\n",
    "# Azure OpenAI setup\n",
    "azure_openai_key = os.getenv(\"AZURE_OPENAI_KEY\")\n",
    "azure_openai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "azure_deployment = os.getenv(\"AZURE_OPENAI_DEPLOYMENT\", \"gpt-4o-mini\")\n",
    "azure_embedding_deployment = os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\", \"text-embedding-ada-002\")\n",
    "azure_openai_api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2024-12-01-preview\")  # Update to match your endpoint\n",
    "\n",
    "# Initialize the asynchronous OpenAI client with proper Azure configuration\n",
    "client = AsyncAzureOpenAI(\n",
    "    azure_endpoint=azure_openai_endpoint,\n",
    "    azure_deployment=azure_deployment,\n",
    "    api_key=azure_openai_key,\n",
    "    api_version = azure_openai_api_version\n",
    ")\n",
    "\n",
    "embedding_client = AsyncAzureOpenAI(\n",
    "    azure_endpoint=azure_openai_endpoint,\n",
    "    azure_deployment=azure_embedding_deployment,\n",
    "    api_key=azure_openai_key,\n",
    "    api_version = azure_openai_api_version\n",
    ")\n",
    "\n",
    "# Create a Semantic Kernel instance\n",
    "kernel = Kernel()\n",
    "chat_completion_service = OpenAIChatCompletion(\n",
    "    ai_model_id=azure_deployment,\n",
    "    async_client=client,\n",
    "    service_id=\"agent\",\n",
    ")\n",
    "kernel.add_service(chat_completion_service)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6677b2c8",
   "metadata": {},
   "source": [
    "## Knowledge Base Creation with Azure AI Search\n",
    "\n",
    "Let's explore in more detail how Azure AI Search functions as a knowledge base for our system:\n",
    "\n",
    "### Key Components of an Azure AI Search Knowledge Base\n",
    "\n",
    "1. **Data Source Connection**: \n",
    "   - Azure AI Search connects to various data sources, including blob storage, SQL databases, and Cosmos DB\n",
    "   - In our example, we directly parsed a PDF document into text\n",
    "\n",
    "2. **Indexing Pipeline**:\n",
    "   - **Extraction**: Converting documents into text (e.g., extracting from PDFs)\n",
    "   - **Chunking**: Breaking down documents into smaller, manageable pieces\n",
    "   - **Enrichment**: Adding metadata, entity extraction, or image analysis\n",
    "   - **Normalization**: Transforming text for better search (lowercasing, lemmatization)\n",
    "\n",
    "3. **Search Index**:\n",
    "   - **Fields**: Structured data like title, content, page number\n",
    "   - **Analyzers**: Language-specific processing for better text matching\n",
    "   - **Scoring Profiles**: Customizing relevance based on specific fields or freshness\n",
    "\n",
    "4. **Query Types**:\n",
    "   - **Keyword Search**: Direct matching of terms (BM25 algorithm)\n",
    "   - **Semantic Search**: Understanding query intent (requires AI models)\n",
    "   - **Vector Search**: Finding similar concepts using embeddings\n",
    "   - **Filters**: Narrowing results by metadata (e.g., document category)\n",
    "\n",
    "### Why Azure AI Search Excels for Knowledge Bases\n",
    "\n",
    "- **Scale**: Handles millions of documents efficiently\n",
    "- **Relevance**: Sophisticated ranking algorithms ensure most relevant content appears first\n",
    "- **AI Integration**: Built-in natural language processing capabilities\n",
    "- **Security**: Role-based access control and document-level security\n",
    "- **Real-time**: Index updates appear in search results immediately\n",
    "\n",
    "In our agentic RAG system, Azure AI Search forms the foundation of the knowledge retrieval process, allowing the agent to quickly find and leverage the most relevant information from the employee handbook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d178eb",
   "metadata": {},
   "source": [
    "## 3. Document Indexing with Azure AI Search\n",
    "\n",
    "Let's set up our document indexing pipeline using Azure AI Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26dbc6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants for index\n",
    "INDEX_NAME = \"hr-documents\"\n",
    "MAX_TOKENS_PER_CHUNK = 1000\n",
    "MAX_CHUNKS_PER_DOC = 10\n",
    "VECTOR_DIMENSIONS = 1536  # Dimensions for text-embedding-ada-002\n",
    "\n",
    "# Define the schema for our search index\n",
    "def create_search_index(index_name: str, index_client: SearchIndexClient):\n",
    "    \"\"\"Create a search index if it doesn't exist.\"\"\"\n",
    "    \n",
    "    if index_name in [index.name for index in index_client.list_indexes()]:\n",
    "        print(f\"Index '{index_name}' already exists\")\n",
    "        return\n",
    "    \n",
    "    fields = [\n",
    "        SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True),\n",
    "        SearchableField(name=\"content\", type=SearchFieldDataType.String, analyzer_name=\"en.microsoft\"),\n",
    "        SimpleField(name=\"title\", type=SearchFieldDataType.String),\n",
    "        SimpleField(name=\"category\", type=SearchFieldDataType.String),\n",
    "        SimpleField(name=\"page_num\", type=SearchFieldDataType.Int32),\n",
    "        SearchField(\n",
    "            name=\"vector\", \n",
    "            type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "            vector_search_dimensions=VECTOR_DIMENSIONS,\n",
    "            vector_search_profile_name=\"vector-profile\"\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    vector_search = VectorSearch(\n",
    "        algorithms=[\n",
    "            HnswAlgorithmConfiguration(\n",
    "                name=\"vector-algorithm\", \n",
    "                kind=VectorSearchAlgorithmKind.HNSW\n",
    "            )\n",
    "        ],\n",
    "        profiles=[\n",
    "            VectorSearchProfile(\n",
    "                name=\"vector-profile\", \n",
    "                algorithm_configuration_name=\"vector-algorithm\"\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    index = SearchIndex(name=index_name, fields=fields, vector_search=vector_search)\n",
    "    index_client.create_index(index)\n",
    "    print(f\"Created index '{index_name}' with vector search capability\")\n",
    "\n",
    "# Initialize search clients\n",
    "search_index_client = SearchIndexClient(\n",
    "    endpoint=search_endpoint,\n",
    "    credential=AzureKeyCredential(search_admin_key)\n",
    ")\n",
    "\n",
    "search_client = SearchClient(\n",
    "    endpoint=search_endpoint,\n",
    "    index_name=INDEX_NAME,\n",
    "    credential=AzureKeyCredential(search_admin_key)\n",
    ")\n",
    "\n",
    "# Create search index\n",
    "create_search_index(INDEX_NAME, search_index_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacbd62c",
   "metadata": {},
   "source": [
    "## 4. Processing the Employee Handbook PDF with Embeddings\n",
    "\n",
    "Now, let's process the employee handbook PDF file located in the docs folder, generate embeddings, and prepare it for indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e4bb34",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Function to generate embeddings using Azure OpenAI\n",
    "async def generate_embeddings(text):\n",
    "    \"\"\"Generate embeddings for a text using Azure OpenAI.\"\"\"\n",
    "    try:\n",
    "        # Make direct API call to Azure OpenAI\n",
    "        response = await embedding_client.embeddings.create(\n",
    "            input=text,\n",
    "            model=azure_embedding_deployment\n",
    "        )\n",
    "        return response.data[0].embedding\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating embeddings: {e}\")\n",
    "        # Return a zero vector if there's an error\n",
    "        return [0.0] * VECTOR_DIMENSIONS\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text from a PDF file, returning the text content by page.\"\"\"\n",
    "    \n",
    "    print(f\"Extracting text from {pdf_path}...\")\n",
    "    \n",
    "    pdf_pages = []\n",
    "    \n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as file:\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "            \n",
    "            for page_num, page in enumerate(pdf_reader.pages):\n",
    "                text = page.extract_text()\n",
    "                if text.strip():  # Only add non-empty pages\n",
    "                    pdf_pages.append({\n",
    "                        \"page_num\": page_num + 1,\n",
    "                        \"content\": text.strip()\n",
    "                    })\n",
    "            \n",
    "            print(f\"Successfully extracted text from {len(pdf_pages)} pages\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from PDF: {e}\")\n",
    "    \n",
    "    return pdf_pages\n",
    "\n",
    "async def chunk_text_with_embeddings(pages, max_chunk_size=4000):\n",
    "    \"\"\"Split page content into smaller chunks for better indexing and retrieval and add embeddings.\"\"\"\n",
    "    \n",
    "    chunks = []\n",
    "    \n",
    "    for page in pages:\n",
    "        page_text = page[\"content\"]\n",
    "        page_num = page[\"page_num\"]\n",
    "        \n",
    "        # If the page text is shorter than max_chunk_size, keep it as is\n",
    "        if len(page_text) <= max_chunk_size:\n",
    "            # Generate embedding for the chunk\n",
    "            embedding = await generate_embeddings(page_text)\n",
    "            \n",
    "            chunks.append({\n",
    "                \"page_num\": page_num,\n",
    "                \"content\": page_text,\n",
    "                \"title\": f\"Employee Handbook - Page {page_num}\",\n",
    "                \"category\": \"Handbook\",\n",
    "                \"vector\": embedding\n",
    "            })\n",
    "        else:\n",
    "            # Split by paragraphs first\n",
    "            paragraphs = page_text.split('\\n\\n')\n",
    "            current_chunk = \"\"\n",
    "            \n",
    "            for para in paragraphs:\n",
    "                if len(current_chunk) + len(para) <= max_chunk_size:\n",
    "                    current_chunk += para + \"\\n\\n\"\n",
    "                else:\n",
    "                    # Add the current chunk if it's not empty\n",
    "                    if current_chunk:\n",
    "                        # Generate embedding for the chunk\n",
    "                        embedding = await generate_embeddings(current_chunk.strip())\n",
    "                        \n",
    "                        chunks.append({\n",
    "                            \"page_num\": page_num,\n",
    "                            \"content\": current_chunk.strip(),\n",
    "                            \"title\": f\"Employee Handbook - Page {page_num}\",\n",
    "                            \"category\": \"Handbook\",\n",
    "                            \"vector\": embedding\n",
    "                        })\n",
    "                    \n",
    "                    current_chunk = para + \"\\n\\n\"\n",
    "            \n",
    "            # Add the last chunk if it's not empty\n",
    "            if current_chunk:\n",
    "                # Generate embedding for the chunk\n",
    "                embedding = await generate_embeddings(current_chunk.strip())\n",
    "                \n",
    "                chunks.append({\n",
    "                    \"page_num\": page_num,\n",
    "                    \"content\": current_chunk.strip(),\n",
    "                    \"title\": f\"Employee Handbook - Page {page_num}\",\n",
    "                    \"category\": \"Handbook\",\n",
    "                    \"vector\": embedding\n",
    "                })\n",
    "    \n",
    "    print(f\"Created {len(chunks)} chunks with embeddings from {len(pages)} pages\")\n",
    "    return chunks\n",
    "\n",
    "# Index documents\n",
    "def index_documents(documents, search_client):\n",
    "    \"\"\"Index a list of documents into Azure AI Search.\"\"\"\n",
    "    \n",
    "    indexed_docs = []\n",
    "    \n",
    "    for doc in documents:\n",
    "        # Create a unique ID for each document\n",
    "        doc_id = str(uuid.uuid4())\n",
    "        \n",
    "        # Format the document for indexing\n",
    "        search_doc = {\n",
    "            \"id\": doc_id,\n",
    "            \"title\": doc[\"title\"],\n",
    "            \"content\": doc[\"content\"],\n",
    "            \"category\": doc[\"category\"],\n",
    "            \"page_num\": doc[\"page_num\"],\n",
    "            \"vector\": doc[\"vector\"]\n",
    "        }\n",
    "        \n",
    "        indexed_docs.append(search_doc)\n",
    "    \n",
    "    # Index the documents in batches\n",
    "    search_client.upload_documents(documents=indexed_docs)\n",
    "    print(f\"Indexed {len(indexed_docs)} documents with vector embeddings\")\n",
    "    \n",
    "    return indexed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1cb966",
   "metadata": {
    "title": "Process the Employee Handbook PDF"
   },
   "outputs": [],
   "source": [
    "async def process_and_index_pdf():\n",
    "    pdf_path = \"docs/contoso_electronics.pdf\"\n",
    "    pdf_pages = extract_text_from_pdf(pdf_path)\n",
    "    pdf_chunks = await chunk_text_with_embeddings(pdf_pages)\n",
    "    \n",
    "    # Index the processed chunks\n",
    "    indexed_documents = index_documents(pdf_chunks, search_client)\n",
    "    return indexed_documents\n",
    "\n",
    "# Setup function for initializing vector search\n",
    "async def init_vector_search():\n",
    "    # Process and index the PDF\n",
    "    await process_and_index_pdf()\n",
    "    print(\"Vector search initialized!\")\n",
    "    return True\n",
    "\n",
    "# When running in Jupyter, you can initialize with:\n",
    "await init_vector_search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdc4a45",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Define a function to search HR documents using vector search\n",
    "async def search_hr_documents(query, top=5):\n",
    "    \"\"\"\n",
    "    Search for HR documents based on a query using vector search.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The search query\n",
    "        top (int): Number of top results to return\n",
    "        \n",
    "    Returns:\n",
    "        str: Formatted search results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Generate embedding for the query\n",
    "        query_embedding = await generate_embeddings(query)\n",
    "        \n",
    "        # Perform vector search\n",
    "        vector_results = search_client.search(\n",
    "            search_text=query,\n",
    "            vector_queries=[{\n",
    "                \"kind\": \"vector\",\n",
    "                \"vector\": query_embedding,\n",
    "                \"k\": top,\n",
    "                \"fields\": \"vector\"\n",
    "            }],\n",
    "            select=[\"title\", \"content\", \"page_num\"],\n",
    "            top=top\n",
    "        )\n",
    "        # Format the results\n",
    "        results_text = f\"Search results for: '{query}'\\n\\n\"\n",
    "        \n",
    "        for i, result in enumerate(vector_results):\n",
    "            results_text += f\"Result {i+1} (Page {result['page_num']}):\\n\"\n",
    "            results_text += f\"Title: {result['title']}\\n\"\n",
    "            results_text += f\"Content: {result['content'][:200]}...\\n\\n\"\n",
    "        \n",
    "        return results_text\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error performing search: {str(e)}\"\n",
    "\n",
    "# Let's test the vector search\n",
    "await search_hr_documents(\"What are the company values?\", top=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3cecd6",
   "metadata": {},
   "source": [
    "## 5. Creating Plugins for the Agentic RAG System\n",
    "\n",
    "Now, let's define a simple search plugin that our agent will use for retrieving information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f19f659",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentSearchPlugin:\n",
    "    \"\"\"A Plugin that provides search capabilities for HR documents.\"\"\"\n",
    "\n",
    "    def __init__(self, search_client):\n",
    "        self.search_client = search_client\n",
    "\n",
    "    @kernel_function(description=\"Search for HR documents based on a query.\")\n",
    "    async def search_hr_documents(\n",
    "        self, \n",
    "        query: str,\n",
    "        top: Optional[int] = 3\n",
    "    ) -> Annotated[str, \"Returns the search results as formatted text.\"]:\n",
    "        \"\"\"Search for HR documents that match the query.\"\"\"\n",
    "        try:\n",
    "            # Generate embedding for the query\n",
    "            query_embedding = await generate_embeddings(query)\n",
    "            \n",
    "            # Perform vector search\n",
    "            vector_results = self.search_client.search(\n",
    "                search_text=query,\n",
    "                vector_queries=[{\n",
    "                    \"kind\": \"vector\",\n",
    "                    \"vector\": query_embedding,\n",
    "                    \"k\": top,\n",
    "                    \"fields\": \"vector\"\n",
    "                }],\n",
    "                select=[\"title\", \"content\", \"page_num\"],\n",
    "                top=top\n",
    "            )\n",
    "            \n",
    "            # Format the results\n",
    "            results_text = f\"Search results for: '{query}'\\n\\n\"\n",
    "            \n",
    "            for i, result in enumerate(vector_results):\n",
    "                results_text += f\"Result {i+1} (Page {result['page_num']}):\\n\"\n",
    "                results_text += f\"Title: {result['title']}\\n\"\n",
    "                results_text += f\"Content: {result['content'][:300]}...\\n\\n\"\n",
    "            \n",
    "            return results_text\n",
    "        \n",
    "        except Exception as e:\n",
    "            return f\"Error performing search: {str(e)}\"\n",
    "\n",
    "# Register the search plugin with the kernel\n",
    "kernel.add_plugin(DocumentSearchPlugin(search_client), plugin_name=\"searchPlugin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69dd2bfe",
   "metadata": {},
   "source": [
    "## 6. Creating a Simple Agentic RAG Assistant\n",
    "\n",
    "Let's create a simple agent that can perform multiple searches as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dfe32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the agent with system message that encourages multiple searches\n",
    "agent = ChatCompletionAgent(\n",
    "    kernel=kernel,\n",
    "    instructions=\"\"\"You are a helpful HR assistant named HRBot that specializes in company policies and procedures.\n",
    "    Your purpose is to answer employee questions accurately using company documentation.\n",
    "    \n",
    "    IMPORTANT SEARCH INSTRUCTIONS:\n",
    "    1. When answering questions, you have access to a searchPlugin.search_hr_documents function.\n",
    "    2. You should make MULTIPLE searches with DIFFERENT search queries to gather comprehensive information.\n",
    "    3. For each question, formulate 2-3 DIFFERENT search queries that approach the question from different angles.\n",
    "    4. Refine your search queries based on initial results - if information is missing, search again with more specific terms.\n",
    "    5. When formulating search queries, use HR terminology and specific policy-related keywords.\n",
    "    \n",
    "    When responding:\n",
    "    - Combine information from all search results to provide complete answers\n",
    "    - Always cite which page of the handbook information comes from\n",
    "    - If information is unavailable after multiple searches, acknowledge this and suggest who to contact\n",
    "    - Be professional, concise, and helpful\n",
    "    \n",
    "    Example search strategy for \"What is the vacation policy?\":\n",
    "    1. First search: \"vacation policy allowance\"\n",
    "    2. Second search: \"paid time off accrual\"\n",
    "    3. Third search: \"requesting vacation procedure\"\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b5f626",
   "metadata": {},
   "source": [
    "## 7. Testing Our Simple Agentic RAG Assistant\n",
    "\n",
    "Let's test our assistant with some example queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9826a5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_agentic_search():\n",
    "    # Create a chat history\n",
    "    chat_history = ChatHistory()\n",
    "\n",
    "    user_inputs = [\n",
    "        \"What benefits am I eligible for and when do they start?\",\n",
    "        \"What is the process for requesting time off or reporting sick leave?\",\n",
    "        \"Where can I find information about the company's dress code policy?\",\n",
    "        \"How does the performance review process work and how often will I be evaluated?\"\n",
    "    ]\n",
    "\n",
    "    for user_input in user_inputs:\n",
    "        # Add the user message to chat history\n",
    "        chat_history.add_user_message(user_input)\n",
    "        \n",
    "        # Display user query\n",
    "        html_output = f\"<p><strong>User:</strong> {user_input}</p>\"\n",
    "        \n",
    "        agent_name: str | None = None\n",
    "        full_response = \"\"\n",
    "        function_calls = []\n",
    "        function_results = {}\n",
    "        \n",
    "        # Track function calls by their ID and accumulate arguments\n",
    "        function_call_accumulator = {}\n",
    "\n",
    "        # Collect the agent's response\n",
    "        async for content in agent.invoke_stream(chat_history):\n",
    "            if not agent_name and hasattr(content, 'name'):\n",
    "                agent_name = content.name\n",
    "\n",
    "            # Track function calls and results\n",
    "            for item in content.items:\n",
    "                if isinstance(item, FunctionCallContent):\n",
    "                    # Get or create accumulator for this function call\n",
    "                    call_id = getattr(item, 'id', None) or str(uuid.uuid4())\n",
    "                    \n",
    "                    if call_id not in function_call_accumulator:\n",
    "                        function_call_accumulator[call_id] = {\n",
    "                            'function_name': item.function_name,\n",
    "                            'arguments': '',\n",
    "                            'processed': False\n",
    "                        }\n",
    "                    \n",
    "                    # Accumulate arguments\n",
    "                    function_call_accumulator[call_id]['arguments'] += item.arguments\n",
    "                    \n",
    "                    # Try to parse complete JSON\n",
    "                    try:\n",
    "                        args = json.loads(function_call_accumulator[call_id]['arguments'])\n",
    "                        if not function_call_accumulator[call_id]['processed']:\n",
    "                            query = args.get(\"query\", \"\")\n",
    "                            call_info = f\"Calling: search_hr_documents(query=\\\"{query}\\\")\"\n",
    "                            function_calls.append(call_info)\n",
    "                            function_call_accumulator[call_id]['processed'] = True\n",
    "                    except json.JSONDecodeError:\n",
    "                        # JSON not complete yet, continue accumulating\n",
    "                        pass\n",
    "                        \n",
    "                elif isinstance(item, FunctionResultContent):\n",
    "                    result_info = f\"Result: {item.result[:150]}...\" if len(item.result) > 150 else f\"Result: {item.result}\"\n",
    "                    function_calls.append(result_info)\n",
    "                    # Store function results to add to chat history\n",
    "                    function_results[item.function_name] = item.result\n",
    "\n",
    "            # Extract the text content\n",
    "            if hasattr(content, 'content') and content.content and content.content.strip():\n",
    "                # Check if this is a regular text message (not function related)\n",
    "                if not any(isinstance(item, (FunctionCallContent, FunctionResultContent))\n",
    "                         for item in content.items):\n",
    "                    full_response += content.content\n",
    "\n",
    "        # Add function calls to HTML\n",
    "        if function_calls:\n",
    "            html_output += '<details><summary style=\"cursor: pointer; font-weight: bold;\">Search Queries (click to expand)</summary><pre>'\n",
    "            html_output += \"\\n\".join(function_calls)\n",
    "            html_output += '</pre></details>'\n",
    "\n",
    "        # Add agent response to HTML\n",
    "        html_output += f\"<p><strong>{agent_name or 'HRBot'}:</strong> {full_response}</p>\"\n",
    "        html_output += \"<hr>\"\n",
    "\n",
    "        # Add agent's response to chat history\n",
    "        if full_response:\n",
    "            chat_history.add_assistant_message(full_response)\n",
    "\n",
    "        # Display formatted HTML\n",
    "        display(Markdown(html_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65b1f1f",
   "metadata": {},
   "source": [
    "Now let's test our RAG agent with some example queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a4be10",
   "metadata": {},
   "outputs": [],
   "source": [
    "await test_agentic_search()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d95d004",
   "metadata": {},
   "source": [
    "It's time to try and prompt the agent yourself with your own questions. Try creating other queries to see how the agent performs. Feel free to modify the system prompt to see if you can improve the agent's performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d938ca5",
   "metadata": {},
   "source": [
    "## 8. Conclusion\n",
    "\n",
    "In this challenge, we've explored how to build an agentic RAG system using Azure AI Search and Semantic Kernel. We've learned:\n",
    "\n",
    "1. How to extract and process text from PDF documents\n",
    "2. How to index documents in Azure AI Search\n",
    "3. How to create plugins for document retrieval and query refinement\n",
    "4. How to integrate retrieval mechanisms with a conversational agent\n",
    "5. How to improve search results through query refinement\n",
    "\n",
    "This agentic approach transforms RAG from a simple lookup mechanism into an intelligent system that can handle nuanced information needs by:\n",
    "\n",
    "- **Dynamically refining queries** to improve search results\n",
    "- **Selectively retrieving information** based on the user's needs\n",
    "- **Integrating retrieval with other capabilities** through the plugin system\n",
    "\n",
    "These concepts can be applied to various enterprise scenarios, particularly for building knowledge bases that help employees navigate company policies and procedures. "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
